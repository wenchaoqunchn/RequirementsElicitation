# Approach Reproduction Source Code

This folder contains the source code to reproduce the **LLM-based Evolutionary Requirements Elicitation** method described in the paper. It provides an end-to-end pipeline from the anonymized dataset (`anonymous_data`) to inferred requirements.

## Project Structure

```
tool_src/
├── config.py             # Configuration settings (paths, API keys, thresholds)
├── data_loader.py        # Handles loading of JSON sequences and video paths
├── anomaly_detector.py   # Implements rule-based anomaly detection (e.g., repetitive clicks)
├── context_builder.py    # Extracts video frames and constructs LLM prompts
├── llm_client.py         # Interface for LLM interaction (Mocked for safety, uncomment for real usage)
├── main.py               # Main execution script
└── output/               # Generated results and cached frames
```

## Prerequisites

1.  **Python 3.8+**
2.  **Dependencies**:
    ```bash
    pip install pandas opencv-python openai openpyxl
    ```
3.  **Dataset**: Ensure the `anonymous_data` folder is located at `../anonymous_data` relative to this folder.

## How to Run

1.  **Configure**: Edit `config.py` to set your preferences.
    - **API Mode**: Set `LLM_INTERACTION_MODE = "API"` and provide `OPENAI_API_KEY`.
    - **Web UI Mode**: Set `LLM_INTERACTION_MODE = "WEB_UI"` (Default). This generates a guide for manual interaction with ChatGPT.

2.  **Execute**:
    ```bash
    python main.py
    ```

3.  **Output**:
    - **API Mode**: Results are saved to `output/inferred_requirements.xlsx`.
    - **Web UI Mode**: A guide is generated at `output/manual_interaction_guide.md`. Open this file to see the extracted frames and pre-generated prompts. You can copy-paste them into the ChatGPT web interface.

## Implementation Details

### 1. Anomaly Detection (`anomaly_detector.py`)
We implement a simplified version of the rule-based detection described in Section III.C of the paper.
- **Repetitive Interactions**: Detects if the same widget is interacted with >= 3 times consecutively.
- **Long Duration**: Detects if a user stays on a page/action for > 5 seconds without progress.

### 2. Context Extraction (`context_builder.py`)
- Uses `opencv` to seek to the exact timestamp of the anomaly in the video recording.
- Saves the frame as a JPEG image.
- Constructs a structured prompt including:
    - **Goal Context**: Defined in `config.py`.
    - **GUI Info**: Reference to the extracted frame.
    - **Anomaly Description**: Generated by the detector.

### 3. LLM Inference (`llm_client.py`)
- Simulates the role of a "Requirement Analyst".
- Uses the prompt structure defined in Table III of the paper.
- **Note**: The default code uses a mock response to avoid consuming API credits. Uncomment the API call lines in `llm_client.py` to use real GPT-3.5/4.

## Extending the Pipeline

To fully replicate the paper's results:
1.  **Refine Task Context**: Update `TASK_DEFINITIONS` in `config.py` with the specific tasks used in your experiment.
2.  **Enhance GUI Analysis**: Integrate an Image-to-Text model (like GPT-4 Vision or a local OCR tool) in `context_builder.py` to generate textual descriptions of the interface from the extracted frames.
3.  **Prioritization**: Implement the UX Score calculation (using `raw_data` metrics) to sort the rows in the final Excel output.
